# Module 3: dbt Fundamentals

**What you learned about data transformation with dbt**

---

## ğŸ¯ Core Concepts Learned

### 1. What is dbt?

**dbt = data build tool**

**Simple explanation you discovered:**
```
Raw Data (in Snowflake)
    â†“
dbt transforms it (using SQL)
    â†“
Clean, modeled data (back in Snowflake)
```

**What dbt does:**
- âœ… Transforms data using SQL
- âœ… Tests data quality
- âœ… Documents models
- âœ… Manages dependencies
- âœ… Versions transformations in Git

**What dbt does NOT do:**
- âŒ Extract data from sources
- âŒ Load data into warehouse
- âŒ Visualize data
- âŒ Replace your data warehouse

**Key insight:** dbt is the "T" in ELT (Extract, Load, Transform)

---

### 2. ELT vs ETL Paradigm Shift

**Old Way (ETL):**
```
Extract â†’ Transform (on separate server) â†’ Load to warehouse
```
- Expensive transformation servers
- Data not immediately queryable
- Hard to debug

**New Way (ELT) - What you used:**
```
Extract â†’ Load to warehouse â†’ Transform (in warehouse with dbt)
```
- Uses warehouse compute (already paying for it)
- All data immediately queryable
- Easy to debug (it's just SQL)

**Why dbt enables ELT:**
- Warehouses are now powerful enough
- Separation of compute/storage makes it affordable
- SQL is the transformation language

---

## ğŸ“ dbt Project Structure You Built

### Directory Layout

```
my_dbt_project/
â”œâ”€â”€ dbt_project.yml       â† Project configuration
â”œâ”€â”€ models/               â† Your SQL transformations
â”‚   â”œâ”€â”€ 01_staging/      â† Clean raw data
â”‚   â”œâ”€â”€ 02_intermediate/ â† Business logic
â”‚   â””â”€â”€ 03_mart/         â† Analytics-ready
â”œâ”€â”€ tests/               â† Custom data tests
â”œâ”€â”€ macros/              â† Reusable SQL functions
â”œâ”€â”€ seeds/               â† Small CSV files
â””â”€â”€ snapshots/           â† Historical tracking
```

---

### Three-Layer Architecture You Implemented

**Why three layers:**

```
01_staging/
    â†“ Purpose: Clean and standardize
    â†“ Input: Raw source data
    â†“ Output: Consistent, typed data
    â†“
02_intermediate/
    â†“ Purpose: Business logic
    â†“ Input: Staging models
    â†“ Output: Joined, enriched data
    â†“
03_mart/
    â†“ Purpose: Analytics-ready
    â†“ Input: Intermediate models
    â†“ Output: Dimensional models (facts, dims)
```

**Real-world benefit:**
- âœ… Separation of concerns
- âœ… Easier to maintain
- âœ… Can reuse staging across marts
- âœ… Clear data lineage

---

## ğŸ”§ Key dbt Components You Used

### 1. Sources - Defining Raw Data

**File:** `models/01_staging/_sources.yml`

**What you wrote:**
```yaml
sources:
  - name: raw
    database: DB_T34
    schema: RAW
    tables:
      - name: customers
```

**What this does:**
- Tells dbt where raw data lives
- Can reference with `{{ source('raw', 'customers') }}`
- Enables dependency tracking
- Enables source freshness checks

**Why use sources vs hardcoding:**
```sql
-- âŒ DON'T: Hardcode table names
FROM DB_T34.RAW.CUSTOMERS

-- âœ… DO: Use source function
FROM {{ source('raw', 'customers') }}
```

**Benefits:**
- Change database/schema in one place
- dbt tracks if source tables exist
- Can test source data quality
- Clear in documentation

---

### 2. Models - Your SQL Transformations

**File:** `models/01_staging/stg_customers.sql`

**What you wrote:**
```sql
{{ config(materialized='view') }}

SELECT
    customer_id,
    UPPER(customer_name) as customer_name,
    LOWER(email) as email,
    CASE
        WHEN total_spent >= 500 THEN 'High Value'
        WHEN total_spent >= 200 THEN 'Medium Value'
        ELSE 'Low Value'
    END as customer_segment
FROM {{ source('raw', 'customers') }}
```

**What dbt does with this:**
1. Reads the SQL file
2. Compiles Jinja (the `{{ }}` parts)
3. Runs the SQL in Snowflake
4. Creates a VIEW called `stg_customers`
5. Stores it in `DB_T34.PUBLIC.STG_CUSTOMERS`

---

### 3. Materialization Strategies

**What you learned:**

**VIEW (what you used):**
```yaml
{{ config(materialized='view') }}
```
- âœ… Always fresh data
- âœ… No storage cost
- âŒ Query runs every time
- **Use when:** Data changes frequently, fast query

**TABLE:**
```yaml
{{ config(materialized='table') }}
```
- âœ… Fast to query
- âŒ Storage cost
- âŒ Data can be stale
- **Use when:** Slow query, data doesn't change often

**INCREMENTAL (you'll learn next):**
```yaml
{{ config(materialized='incremental') }}
```
- âœ… Only processes new data
- âœ… Handles big tables
- âŒ More complex logic
- **Use when:** Millions of rows, append-only data

**Real-world decision tree:**
```
Is data > 1M rows?
â”œâ”€ YES â†’ Use incremental
â””â”€ NO â†’ Is query slow?
    â”œâ”€ YES â†’ Use table
    â””â”€ NO â†’ Use view (what you used)
```

---

### 4. Tests - Data Quality as Code

**File:** `models/01_staging/stg_customers.yml`

**What you wrote:**
```yaml
models:
  - name: stg_customers
    columns:
      - name: customer_id
        tests:
          - unique
          - not_null
      - name: customer_segment
        tests:
          - accepted_values:
              values: ['High Value', 'Medium Value', 'Low Value']
```

**What dbt does with tests:**
```bash
dbt test
```

**Generates SQL like:**
```sql
-- unique test
SELECT customer_id, COUNT(*)
FROM stg_customers
GROUP BY customer_id
HAVING COUNT(*) > 1  -- Fails if any duplicates

-- not_null test
SELECT *
FROM stg_customers
WHERE customer_id IS NULL  -- Fails if any nulls

-- accepted_values test
SELECT *
FROM stg_customers
WHERE customer_segment NOT IN ('High Value', 'Medium Value', 'Low Value')
```

**Why tests matter:**
- ğŸ› Catch data quality issues automatically
- ğŸ“Š Document expectations
- ğŸ”” Alert when data breaks
- âœ… Confidence in prod

---

## ğŸ¨ Jinja Templating You Used

### {{ }} - Execute Python/Jinja

**What you used:**
```sql
FROM {{ source('raw', 'customers') }}
```

**What it becomes:**
```sql
FROM DB_T34.RAW.CUSTOMERS
```

**Other Jinja you'll use:**
```sql
-- If/else logic
{% if target.name == 'prod' %}
    FROM prod_table
{% else %}
    FROM dev_table
{% endif %}

-- Loops
{% for column in ['name', 'email', 'city'] %}
    {{ column }},
{% endfor %}

-- Set variables
{% set payment_methods = ['credit', 'debit', 'paypal'] %}
```

**Why Jinja in SQL:**
- âœ… DRY (Don't Repeat Yourself)
- âœ… Environment-specific logic
- âœ… Generate SQL programmatically
- âœ… Reusable code patterns

---

## ğŸ”„ dbt Workflow You Learned

### Development Cycle

```bash
# 1. Make changes to SQL file
vim models/01_staging/stg_customers.sql

# 2. Run the model
dbt run --select stg_customers

# 3. Test the model
dbt test --select stg_customers

# 4. Check results in Snowflake
# Query the table/view

# 5. Commit to Git
git add . && git commit -m "Add customer staging model"
```

**Key insight:** Fast iteration loop (edit â†’ run â†’ test â†’ verify)

---

### Commands You Mastered

**dbt debug:**
```bash
dbt debug
# Tests connection, shows configuration
```

**dbt run:**
```bash
dbt run                          # Run all models
dbt run --select stg_customers   # Run specific model
dbt run --select 01_staging      # Run all staging models
dbt run --select +stg_customers  # Run upstream dependencies too
```

**dbt test:**
```bash
dbt test                         # Run all tests
dbt test --select stg_customers  # Test specific model
```

**dbt compile:**
```bash
dbt compile
# Shows compiled SQL without running it
# Good for debugging Jinja
```

---

## ğŸ’¡ Key Insights You Discovered

### Insight 1: "SQL + Version Control = Magic"

**What you learned:**
- SQL transformations in Git
- Can see history of changes
- Can collaborate like software engineers
- Can review before deploying

**Real-world impact:**
```bash
git log models/01_staging/stg_customers.sql
# See who changed what and when
```

---

### Insight 2: "Tests Are Documentation"

**Your test:**
```yaml
- name: customer_segment
  tests:
    - accepted_values:
        values: ['High Value', 'Medium Value', 'Low Value']
```

**What this tells you:**
- âœ… customer_segment can only be these three values
- âœ… This is enforced automatically
- âœ… If data breaks this, tests fail

**Better than comment:**
```sql
-- customer_segment should be High/Medium/Low Value
-- (Nobody checks this)
```

---

### Insight 3: "Transformation Layers Enable Collaboration"

**What you built:**
```
01_staging/ â†’ 02_intermediate/ â†’ 03_mart/
```

**Why it matters:**
- Team A owns staging (data engineers)
- Team B owns marts (analytics engineers)
- Both can work independently
- Changes in staging flow through

---

## ğŸš« Common Mistakes You Avoided

### Mistake 1: Complex Logic in One Model
âŒ **DON'T:**
```sql
-- 500 lines of SQL with 10 joins in one file
```

âœ… **DO:**
```sql
-- 01_staging/: Clean raw data (simple)
-- 02_intermediate/: Join and enrich (moderate)
-- 03_mart/: Final dimensional model (simple again)
```

**What you did:** Kept stg_customers simple (just cleaning)

---

### Mistake 2: No Tests
âŒ **DON'T:** Assume data is correct

âœ… **DO:** Test assumptions
```yaml
tests:
  - unique
  - not_null
  - accepted_values
```

**What you did:** 8 tests on first model âœ…

---

### Mistake 3: Hardcoding Database Names
âŒ **DON'T:**
```sql
FROM DB_T34.RAW.CUSTOMERS
```

âœ… **DO:**
```sql
FROM {{ source('raw', 'customers') }}
```

**What you did:** Used source function âœ…

---

## ğŸ“Š dbt Run Lifecycle

### What Happens When You Run `dbt run`

**Step-by-step:**

1. **Parse project:**
   - Read dbt_project.yml
   - Find all .sql files
   - Read all .yml files

2. **Compile Jinja:**
   - Replace `{{ source() }}` with actual table names
   - Process {% if %} statements
   - Generate pure SQL

3. **Build DAG:**
   - Determine dependencies
   - Order models correctly
   - Plan execution

4. **Execute:**
   - Run SQL in Snowflake
   - Create views/tables
   - Collect metrics

5. **Report:**
   - Show results (PASS/FAIL)
   - Execution time
   - Rows affected

**You saw this:**
```
1 of 1 START sql view model PUBLIC.stg_customers ....... [RUN]
1 of 1 OK created sql view model PUBLIC.stg_customers .. [SUCCESS 1] in 0.61s
```

---

## ğŸ¯ Skills You Can Transfer

### To Other Data Platforms
- âœ… dbt works with BigQuery, Redshift, Postgres, etc.
- âœ… Same project structure
- âœ… Same commands
- âœ… Just change profiles.yml

### To Software Engineering
- âœ… Version control for data
- âœ… Testing as code
- âœ… CI/CD for analytics
- âœ… Code review for SQL

### To Analytics Engineering
- âœ… Dimensional modeling
- âœ… Incremental processing
- âœ… Data quality frameworks
- âœ… Documentation generation

---

## ğŸ“ Quiz Yourself

1. **What is the difference between a source and a model?**
   <details>
   <summary>Answer</summary>
   Source = raw data (already exists in warehouse). Model = transformed data (created by dbt)
   </details>

2. **When should you use a view vs a table?**
   <details>
   <summary>Answer</summary>
   View = always fresh, fast to build. Table = faster to query, uses storage. Use view for small/fast queries, table for large/slow queries.
   </details>

3. **Why test your models?**
   <details>
   <summary>Answer</summary>
   Catch data quality issues automatically, document expectations, alert when data breaks
   </details>

4. **What does {{ source('raw', 'customers') }} do?**
   <details>
   <summary>Answer</summary>
   References the customers table defined in sources.yml, compiles to actual database.schema.table
   </details>

---

## ğŸš€ What You Can Do Now

âœ… Create dbt models with SQL
âœ… Define sources for raw data
âœ… Write data quality tests
âœ… Use Jinja templating basics
âœ… Run and test models
âœ… Understand dbt execution lifecycle
âœ… Apply three-layer architecture

---

## ğŸ“ˆ Your Progress

**What you built:**
- 1 source definition
- 1 staging model
- 8 automated tests
- Complete documentation

**What this represents:**
- Foundation for entire dbt project
- Repeatable pattern for more models
- Professional-grade data pipeline

---

**Key Takeaway:** dbt transforms SQL from ad-hoc scripts into a software engineering workflow. With version control, testing, and documentation built in, you've learned how modern data teams build reliable, scalable data pipelines. The pattern you learned (source â†’ model â†’ test â†’ document) applies to every model you'll ever create.
